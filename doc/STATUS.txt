==============================================================================
interface TextBase {
	Tokenizer getTokenizer();
	// 
	// document access routines;	 
	public void loadDocument(String docId,String docContent);
}

class TextBaseLevelManager {
        // 				
	retokenize(Tokenizer tok,"parentLevel","childLevel");
	TextBaseMapping getTextBase("levelName");
	TextBaseMapping getMap("srcLevel","dstLevel");
}

interface TextBaseMapping {
	  Span mapSpan(Span oldSpan);	
}

abstract class OneStepMapping implements TextBaseMapping {

	abstract void mapPlace(String docId, int charOffset);
	abstract String getMappedDocId();
	abstract Span getMappedDocSpan();
	abstract String getMappedOffset();
		
	// return span in next text base with correspondoing char offsets
	final public Span mapSpan(Span oldSpan)
	{
		// return corresponding span in textBase that you are
		// mapping to, using getMappedDocId
		mapPlace( oldSpan.getDocumentId(), oldSpan.getTextToken(0).charLo() );
		String idLo = getMappedDocId();
		int offsetLo = getMappedOffset();
		mapPlace( oldSpan.getDocumentId(), oldSpan.getTextToken(oldSpan.size()-1).charHi() );
		String idHi = getMappedDocId();
		int offsetHi = getMappedOffset();		
		// make sure idHi==idLo
		Span docSpan = getMappedDocSpan().charIndexSubSpan( offsetLo, offsetHi);
	}
}


==============================================================================

DICTIONARIES
 - have case-sensitive and case-insenstive dictionaries
 - have a(dict) fold case or not, depending on dict type


CONFIDENCE FOR EXTRACTORS

classify.sequential:
 SequenceClassifier {
  // replace null values in prediction with the best classifications 
  // that are consistent with the non-null values
  void consistentlyClassify(ClassLabel[] prediction,Instance[] seq);

  basic idea: in beam searcher, discard candidates that are inconsistent
  with constraints, as they are generated.

 }
 CMM {
  // needs to implement this
 }

// this wraps a text.learn.SequenceAnnotatorLearner.SequenceAnnotator
// and adds confidences
text.learn.ConfidenceReportingSequenceAnnotator implements ExtractorAnnotator,Serializable,Visible extends AbstractAnno
{
 doAnnotate {
   Span.Looper i = labels.getTextBase().documentSpanIterator();
   while (i.hasNext() ) {
       Span s = i.nextSpan();
       String documentId = ...
       MonotonicSubTextLabels tmpLabels = ... // contains just this document
       Instance[] sequence = new Instance[s.size()];
       for (int j=0; j<s.size(); j++) {
           Span tokenSpan = s.subSpan(j,1);
           sequence[j] = fe.extractInstance(labels,tokenSpan);
       }
       ClassLabel[] classLabels = seqClassifier.classification( sequence );
       double confidence = ...
       for (int j=0; j<classLabels.length; j++) {
           labels.setProperty( s.getToken(j), reduction.getTokenProp(), classLabels[j].bestClassName() );
	   reduction.extractFromTags( annotationType, tmpLabels );
	   for each annotationType label in tmpLabels {
	       use consistentlyClassify to find best classification consistent with NOT including this extraction
	       add the correct classification to labels
	   }
       }
   }
 }
}
 

RANKING ISSUES

 Simple approach: like multi
  - classify.rank includes RankingLearner, which extends BatchBinaryClassifierLearner
  batchTrain does a split into subpops and then does batchTrain(subpops) - which might be
  a sequenceDataset?
  - RankingEvaluation is a new class (should we include abstract type of evaluation? or just
  build the intelligence to build the correct type into Tester? 
  - GraphSearchEval will extend (or wrap) a RankingEvaluation

 There are now a bunch of parallel train/test/eval routines:

 classify (Dataset, ClassifierTeacher, ClassifierLearner, Classifier, Evaluation)
 classify.sequential (SequenceDataset, SequenceClassifierTeacher, BatchSequenceClassifierLearner, 
		      SequenceClassifier, and same Evaluation)
 classify.multi (same everything)
 classify.rank would be yet another...
 learn.text. ....
 
 Each includes: Teacher, Learner, Performer, TrainingData, Splitter, Evaluation
 can we have generic algorithms for training, CV-based testing, etc?
 

PROPOSED EXTENSION TO HANDLE RETOKENIZED TEXTBASES:

 in text: 
   - labeled pointer from parent textBase to spanType/retokenized textbases

 in SpanFE: 
   - from(s,textLabels).toLevel("foo") returns corresponding span s'/textLabels' on level "foo"

 MIXUP_COMMAND -> onLevel name;   // push level
 MIXUP_COMMAND -> offLevel;       // back to parent level
 // alternative might be "onLevel foo { .... } which produces a mixup program
 // that will run on the child level named "foo"
 MIXUP_COMMAND -> defLevel name: LEVELDEF+ ;  // produce new child textbase
 LEVELDEF -> from DOCSPANTYPE 
          | token ( re RE | class CLASS | filter MIXUP_TOKENEXPR )
          | pseudoToken ATOM=SPANTYPE(, ATOM=SPANTYPE)+
 GENERATOR -> importFromLevel name SPANTYPE=SPANTYPE (,SPANTYPE=SPANTYPE)+

 for example:

 defSpanType personName = ... 

 defLevel abstractLevel: pseudoToken person=personName, company=companyName;


PRIORITIES:
 - SVMs: update svmlib to get confidences?
 - fix bug in SVM save/restore
 - stderr on error rates/f1's -done, sortof

BUGS:
  - does showTestDetails change the result?
  - should precision/recall start at something other than 1.0 (max precision?)
  - graph doesn't come out if there is no range?
  - spanClosures are not stored properly (only by document); should really store tokenProps also,
  and CWA isn't even tracked for span properties
  - span diff fails for nested spans, eg "a b c <x>d e<x>f g</x> h i j</x>k"

UI tweaks:
  - in TextBaseViewer, an 'apply' button?
  - stop task?
  - can we save all edited changes to an experiment as a config file?
  - should sequenceLearner mark up the environment shown to the user?
  - progress counter & threads for dataset loading?   

LARGER PROJECTS:

 - bootstrapping 
 - pruning & shrinkage for maxent & CRFs
 - heirarchical learning

MEDIUM TASKS:

 - POS tagger

CLEANUPS/DESIGN:

 - clean up AnnotationExample - should some of code go in Extraction2TaggingReduction?
 - should I have default FE's in SequenceAnnotatorLearner,
 SegmentAnnotatorLearner, ClassifierAnnotatorLearner?  that are called
 by UI.Recommended
 - clean up util.Loader?
 - linkage issues: do classify, util compile separately?
 - remove apps/ksteppe, broken demos, BatchInsideOutsideLearner,BatchFilteredFinderLearner,BinaryExample,
    InfoGainInstanceTransform
 - transform.MaskedInstance could be more efficient
 - classify.algorithm.random, classify.algorithm.active are misplaced
 - explanations are messy in text - structure them?
 - edo code: Spiltter, punk => grep, get rid of stopwords in file?
 - Conventions to get names consistently used.

 To evaluate:
 - should Hyperplane extend MutableInstance ?
 - separate out feature extractor loading from classifier loading somehow? 

CHANGES from William:

 7/4/2004: 
 - bug? in maxent fixed?
 - bug in adaboost fixed
 - fixed the "unedited" checkbox so that it shows items
 - generic Saveable interface, for "Save As..." in gui? 
 - made MaxEntLearner's output Visible 
 - cleaned up FancyLoader so that it doesn't always try and parse xml tags
 - messed with the UIMain console a bit
 - fixed bugs in viewer, added better support for saving datasets and stuff
 - defSpanType now does an implicit declareType of its span
 - added comments to script/setup.sh
 - made "Feature(String s)" split on .'s not spaces
 - added LeaveOneOutDictionary stuff
 - set default #epochs to 1, not 4
 - added completeTraining() call in classifiers
 - modified GenericCollinsLearner to be more like CollinsPerceptronLearner
 - fix evaluation for spanProp - added micro-averaged precision/recall
 - fixed -test for TrainTestExtractor
 - ZoomedTextLabelsViewer
 - set span properties with TextLabelsLoader
 - specify span prop with TextLabelsExperiment
 - subsetting crossvalsplitter
 - add SequenceAnnotatorLearner option to show dataset
 -config FILE for CommandLineProcessor
 - added some stats to give more informative errors when dataset is not binary
 - added spanDiffing to TextLabelViewer.ViewerControls
 - added multi-class text categorization, refactored text cat fe's to	
 be able to load mixup files
 - touched up FilteredFinderLearner, added training to distances in ConditionalSemiMarkovModel.dictFE, added
 options to TextLabelsExperiment
 - warning issues on on empty directory files too
 - more visualization stuff in text.learn
 - set up ConditionalSemiMarkovModel.CSMMFE to extend SampleFE.ExtractionFE
 - refactored SampleFE.ExtractionFE to extend SpanFE
 - exposed getSpanFeatureExtractor() in AnnotatorLearner
 - added/documented some more parameters for TextLabelsExperiment
 - added more visualization for CSMM
 - added (some minimal) visualization for CSMM
 - bug fix in VotedPerceptron
 - TFIDF transform -done
 - added textEnv.require("annotationType") - which would throw an error for immutable text environment
 - changed BeamSearch semantics: if there are two history-equivalent entries with different scores, 
 the one with a lower score is dropped
 - dictionary loading is inefficient - changed MixupInterpreter to store dictionaries and tries.
 - MixupProgram now takes streams, and uses library-loading to load relative to classpath
 - added (failing) SVM extraction test
 - space issues for saved Evaluations fixes
 - removed duplication of learning in 'StackedLearner'
 - sliders for SpanViewer.TextViewer
 - add progress counters to tester
 - classifiedSequenceDataset
 - fixed bug in txt.gui.SpanLabeler
 - added charIndexProperSubSpan(lo,hi)
 - added save in GUI frames
 - added WizardUI
 - optimized ParallelViewer
 - changed recieveContent to setContent
 - finished extraction test
 - fixed einat's bug, more work on cls.seq
 - added jwf.jar, jwf-docs to docs, src TypeSelector.java
 - CrossValidatedDataset (builds on classified dataset)
 - FixedTestSetSplitter, for a single train/test partition
 - Preparing to cleanup & unify evaluation code for extraction:
   - remove cls.expt.DatasetSplitter -done, need to cvs
   - move Splitter interface into cls -done, need to cvs
   - added cls.generic interfaces, attached interfaces to node - done, need to cvs cls.generic
  - code to do span-level precision/recall added to SpanDifference, textbaseViewer      
  - must batch load Container (changed TextBase API); significant performance enhancement
  - added Container class representing top level container or document
  - made classifier tests a bit more forgiving
  - added SmartVanillaViewer, integrated with Dataset so that classifications can be seen
  - visible decision trees & boosted classifiers
  - extend evaluation to non-binary learning tasks
  - add ExampleSchema inference to dataset creation, pass schemas in to learners
  - added a ControlableViewer, which can interrogate a toolbar and
  - added a ControlPanel Toolbar specialization, which exports
  - added a ControlledViewer, which couples as a ControlableViewer and a ControlPanel.
  - cleaned up Binary/KWay example mess
  - cls.expt.ClassifiedDataset added
  - added more output to evaluation code, started ClassifiedDataset.java, fixed maxF1 bug
  - StackedLearner, extended form of calibrated learner
  - added reset to classifiers to fix cross-validation bug
  - added serialization utilities
  - made classifiers serializable
  - added subpopulation control in textbaseloader
  - add language model package in .txt
  - added properties to Evaluations
  - some bug fixes and extra monitoring for running extraction-learning experiments
  - fixed precision/recall curves for Evaluation
  - refactored SplitViewer out of ZoomedViewer
  - added txt.gui.SpanViewer, txt.gui.SpanLooperViewer, txt.gui.MarkupPlan
  - added MessageViewer, started new txt.gui.SpanViewer
  - probably bug fixed in decision tree learner
  - added knn, as simple example of a multi-class learner
  - added conditional markov models							 
  - added active learning
  - dataset is an interface
  - added escaping for dataset chars
  - added file loading scheme for tries
  - added some new viewer code, fixed a list selection bug by using awt events
  - added PoissonLearner, PoissonClassifier, and modified SampleDatasets to add test data for Numeric Features
  - problem with pr graphs fixed - JFreeGraph doesn't like duplicated x values
  - modified importOps so that '-1' is means maximal length
  - moved setAnnotatedBy from textenv to MutableTextEnv
  - added provide, require to mixup
  - added dictionary loading
  - refactored guis, see com.wcohen.util.gui  
  - more documentation & guidelines
  - example of classifier learning - webmaster commands?
  - LogisticRegressor should be a BatchBinaryClassifierLearner
  - add special UnivariateLogisticRegressor subclass with an instance that just holds "x"
  - BatchVersion(OnlineBinaryClassifierLearner learner) - creates dataset & trains
  - keep word counts in SpanFE
  - added calibrated binary classifier, online learner, logistic regression
  - new scoring for classifiers: classLabel() holds all information for scores, as well 
    as isPositive, isNegative, isCorrect()
  - bug fix: mixup debugger - can't add spans until after an "import"
  - decision trees - numeric features
  - refactored the Examples/BinaryExamples/KWayExamples to move
  more functionality into the Example level
  - clean up annotator learner - should have access to labels, which is used in
   training (for FE) and assumed to exist at test time.
  - set up test cases for classifiers and TestPackage
  - splitters are type-safe
  - boosting/decision trees 
  -- ant tests to confirm independence of cls, txt-ann, bb
  -- cls.Instance includes a 'getSubpopulationId' which is an string naming the "subpopulation"
    (strata); use Pradeep's CrossValHandler to handle cross-validation of subpopulation's.
  -- cls.expt.RandomSplitter and new CrossValSplitter respect subpopulation's
  -- cls.expt.SimpleRandomSplitter doesn't respect subpopulation's
  -- re-organized learning code in cls:
   --- Classifier is almost a marker interface, with BinaryClassifier and KWayClassifier being the
       'real' interfaces of interest.  There are similar splits for Example and ClassifierLearner.
   ---- Hyperplanes, VotedPerceptron, NaiveBayes are now in cls.linear
  -- allow optional spanTypes, eg @foo? for optional occurance of foo
  -- implement "defSpanType foo =bar- EXPR
  - Classifier{Learner} is abstract, with KWay and Binary as subclasses
  - generic text progress counter
  - start-end-length annotator
  - allow regex's as an alternative deftype, eg defSpanType foo =bar~ re 'in ([a-z]+)',1
   - simple learning expt for txt.ann & cls
   -- cls.expt includes a generic Splitter object which splits any iterator into folds, train/test splits, 
   etc; 
